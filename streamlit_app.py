import streamlit as st
import google.generativeai as genai

# Configura√ß√£o da API do Gemini usando Streamlit Secrets
genai.configure(api_key=st.secrets["gemini"]["api_key"])

# Configura o t√≠tulo e o √≠cone da p√°gina
st.set_page_config(page_title="Chatbot Sommelier", page_icon="üç∑")

st.title("üçá Ol√°! Bem vindo ao Lado V")
st.caption("Sou seu assistente virtual especializado em vinhos.")

# Fun√ß√£o para converter o formato de mensagem do Streamlit para o do Gemini
def convert_messages_to_history(messages):
    """
    Converts Streamlit's message format to Gemini's chat history format.
    The Gemini model expects a list of dictionaries with 'role' and 'parts'.
    """
    history = []
    for message in messages:
        # Gemini usa "model" para o assistente
        role = "model" if message["role"] == "assistant" else "user"
        history.append({"role": role, "parts": [message["content"]]})
    return history

@st.cache_resource(show_spinner=False)
def get_model():
    """
    Fun√ß√£o para inicializar e armazenar o modelo do Gemini.
    """
    model = genai.GenerativeModel(
        model_name="gemini-1.5-flash",
        system_instruction=(
            "Voc√™ √© um sommelier e assistente virtual de uma loja de vinhos. "
            "Seu √∫nico objetivo √© ajudar os clientes a escolher vinhos, "
            "dar sugest√µes de harmoniza√ß√£o e responder a perguntas sobre "
            "vinhos, uvas e regi√µes vin√≠colas. "
            "Se o usu√°rio perguntar algo que n√£o seja sobre vinhos, "
            "voc√™ deve responder de forma educada e respeitosa. "
            "Exemplo: 'Essa √© uma √≥tima pergunta, mas meu foco √© apenas "
            "em vinhos. Como posso te ajudar a encontrar a garrafa perfeita?'"
        )
    )
    return model

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Pergunte-me algo sobre vinhos..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        chat_model = get_model()
        
        # Converte o hist√≥rico da sess√£o para o formato que a API do Gemini entende
        history_for_gemini = convert_messages_to_history(st.session_state.messages)
        
        # Inicia a sess√£o de chat com o hist√≥rico convertido
        chat_session = chat_model.start_chat(history=history_for_gemini)
        
        try:
            # Envia a √∫ltima mensagem do usu√°rio para o chat
            response = chat_session.send_message(prompt)
            st.markdown(response.text)
            
            # Adiciona a resposta do assistente ao hist√≥rico da sess√£o
            st.session_state.messages.append({"role": "assistant", "content": response.text})
        except genai.types.generation_types.BlockedPromptException as e:
            # Captura a exce√ß√£o se a resposta for bloqueada por seguran√ßa
            st.warning("Desculpe, a sua solicita√ß√£o foi bloqueada por raz√µes de seguran√ßa. Tente uma pergunta diferente.")
            # Remove a √∫ltima mensagem para n√£o salvar o prompt bloqueado
            st.session_state.messages.pop()
